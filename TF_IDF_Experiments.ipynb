{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "TF_IDF_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ1qvno3DG7c",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF Experiments with resampling\n",
        "\n",
        "## ML Classification for Records Management\n",
        "\n",
        "Jason Franks\n",
        "\n",
        "Master of Data Science Minor Thesis\n",
        "\n",
        "Supervisors: Dr Greg Rolan, Dr Lan Du"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5KUSU-xDG7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer, balanced_accuracy_score\n",
        "from sklearn.metrics import average_precision_score, auc\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE, SVMSMOTE, RandomOverSampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.combine import SMOTEENN,SMOTETomek\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import io\n",
        "import nltk as nltk\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.mwe import MWETokenizer\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28uH6_m0DVWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EzgAIn0ul76",
        "colab_type": "text"
      },
      "source": [
        "## **Set the following variables to load the data**\n",
        "\n",
        "**mount_path**: path into a google drive to your working folder\n",
        "\n",
        "**data_file**: name of the file containing your data. This must be a tab-separated .tsv file with two columns: 'label', containing the category name, and 'text', containing the record's raw text.\n",
        "\n",
        "Evey category in the data file should have *at least* 10 records.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSKpWqhIDaCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mount_path = '/content/drive/My Drive/'\n",
        "data_file = 'all_docs_trimmed.tsv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJeno6mkDG7j",
        "colab_type": "text"
      },
      "source": [
        "## Import and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ezQOOkucEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_docs = pd.read_csv(mount_path + data_file, \"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miV3saznDG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the labels\n",
        "label_names = all_docs['label'].unique()\n",
        "num_labels = len(list(label_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75qqjn1WDG7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the labels into numbers\n",
        "all_docs['label_i'] = all_docs['label'].astype('category').cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoUUOLhEj5OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions to help assess the output\n",
        "\n",
        "def get_within_category_accuracies( cat_list, cm ):\n",
        "    cat_accuracies = []\n",
        "    for row in range(len(cat_list)):\n",
        "        cm_row = cm[row]\n",
        "        num_correct = cm_row[row]\n",
        "\n",
        "        total = sum(cm[row])\n",
        "        if total == 0:\n",
        "            continue\n",
        "        \n",
        "        cat_accuracies.append(num_correct/total)\n",
        "            \n",
        "    df = pd.DataFrame(zip(cat_list, cat_accuracies), columns=['label', 'accuracy'])\n",
        "    return df\n",
        "\n",
        "def assess_model(test, preds, title, labels, draw_plot=True):        \n",
        "    final_test_accuracy = accuracy_score(test, preds)\n",
        "    final_test_f1 = f1_score(test, preds, average='macro') \n",
        "    final_cat_f1s = f1_score(test, preds, average=None) \n",
        "    final_test_f1_weighted = f1_score(test, preds, average='weighted')    \n",
        "    final_test_precision = precision_score(test, preds, average='macro') \n",
        "    final_cat_precision = precision_score(test, preds, average=None) \n",
        "    final_test_precision_weighted = precision_score(test, preds, average='weighted')    \n",
        "    final_test_recall = recall_score(test, preds, average='macro') \n",
        "    final_cat_recall = recall_score(test, preds, average=None) \n",
        "    final_test_recall_weighted = recall_score(test, preds, average='weighted')    \n",
        "    cm = confusion_matrix(test, preds)\n",
        "\n",
        "    metrics=[]\n",
        "    metrics.append( [\"accuracy\", final_test_accuracy])\n",
        "    metrics.append( [\"f1\", final_test_f1])\n",
        "    metrics.append( [\"f1 weighted\", final_test_f1_weighted])\n",
        "    metrics.append( [\"precision\", final_test_precision])\n",
        "    metrics.append( [\"precision weighted\", final_test_precision_weighted])\n",
        "    metrics.append( [\"recall\", final_test_recall])\n",
        "    metrics.append( [\"recall weighted\", final_test_recall_weighted])\n",
        "\n",
        "    print( \"------------Model assessment-----\")\n",
        "\n",
        "    print( \"test f1 / category, {}\\n\".format( final_cat_f1s))   \n",
        "    print( \"test precision / category, {}\\n\".format( final_cat_precision))   \n",
        "    print( \"test recall / category, {}\\n\".format( final_cat_recall))   \n",
        "    \n",
        "    model_assessment = pd.DataFrame(metrics, columns=[\"metric\", \"value\"])\n",
        "    print(model_assessment)\n",
        "    model_assessment.to_csv(f'{mount_path}/{title}_assess.csv', index=False )\n",
        "\n",
        "    acc_by_cat = get_within_category_accuracies( labels, cm)\n",
        "\n",
        "    acc_by_cat.to_csv(f'{mount_path}/output/{title}_acc_by_cat.csv', index=False )\n",
        "\n",
        "    if draw_plot:\n",
        "      ax = acc_by_cat.plot.bar( x='label', y='accuracy', title=f'{title} Accuracy by Category', legend=None, figsize=(20,20), fontsize=14)\n",
        "      ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "      ax.set_xticklabels(labels, rotation=90, fontsize=12)\n",
        "      plt.tight_layout()\n",
        "      fig = ax.get_figure()\n",
        "      fig.savefig(mount_path + f'/output/{title}_Accuracy_by_Category.png', dpi=300)\n",
        "    \n",
        "    print(\"-------Confusion Matrix---------\")\n",
        "    print(cm)\n",
        "    \n",
        "    cmDF = pd.DataFrame.from_records(cm)    \n",
        "    cmDF.columns=labels\n",
        "    cmDF.index=labels\n",
        "    cmDF.to_csv(f'{mount_path}/output/{title}_cm.csv', index=True)\n",
        "\n",
        "    return acc_by_cat\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6htzcNdoDG7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_names = all_docs['label'].unique()\n",
        "labels = all_docs['label_i'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1UV_qBeyo5N",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize text and find bigrams and trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrkF8d_eDG79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tokenize text, find ngrams, reduce to TF-IDF Vectors\n",
        "df = all_docs[pd.notnull(all_docs['text'])]\n",
        "\n",
        "tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
        "df['text'] = df['text'].str.lower()\n",
        "df['pretext'] = df['text'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpl30sWFDG7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(subset=['text', 'pretext'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9cNUpkVDG8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isNumber(s):    \n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5qdF6dtDG8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hasNumbers(inputString):\n",
        "    return any(char.isdigit() for char in inputString)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTxiS61ODG8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuSy294KDG8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allTokens = []\n",
        "\n",
        "df['pretext'].apply(lambda x: allTokens.extend(x))\n",
        "\n",
        "trigram_measures = TrigramAssocMeasures()\n",
        "finder = TrigramCollocationFinder.from_words(allTokens)\n",
        "# Find all bigrams - every combination of two words\n",
        "trigram_measures = TrigramAssocMeasures()\n",
        "finder = TrigramCollocationFinder.from_words(allTokens)\n",
        "\n",
        "del allTokens\n",
        "df.drop(['pretext'], axis=1)\n",
        "\n",
        "mostFreqTrigrams = finder.nbest(trigram_measures.raw_freq, 500)\n",
        "\n",
        "# Pull out trigrams\n",
        "trigrams = set([trigram for trigram in mostFreqTrigrams if\n",
        "               not isNumber(trigram[0]) and not isNumber(trigram[1]) and not isNumber(trigram[2])\n",
        "               and not (trigram[0] in stopWords) and not (trigram[2] in stopWords) \n",
        "               and (len(trigram[0]) > 2) and (len(trigram[1]) > 2) and (len(trigram[2]) > 2) \n",
        "                and not hasNumbers(trigram[0]) and not hasNumbers(trigram[1]) and not hasNumbers(trigram[2])])\n",
        "\n",
        "print(trigrams)\n",
        "\n",
        "print('Selected [{}] Trigrams.'.format(len(trigrams)))\n",
        "\n",
        "tri_mwe_tokenizer = MWETokenizer(trigrams)\n",
        "\n",
        "df['trigrammed'] = df['pretext'].apply(lambda x: tri_mwe_tokenizer.tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xui5dmcZDG8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allTokens = []\n",
        "\n",
        "df['trigrammed'].apply(lambda x: allTokens.extend(x))\n",
        "\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(allTokens)\n",
        "# Find all bigrams - every combination of two words\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(allTokens)\n",
        "del allTokens\n",
        "\n",
        "mostFreqBigrams = finder.nbest(bigram_measures.raw_freq, 1000)\n",
        "# Pull out bigrams\n",
        "bigrams = set([bigram for bigram in mostFreqBigrams if\n",
        "               not isNumber(bigram[0]) and not isNumber(bigram[1])\n",
        "               and not (bigram[0] in stopWords) and not (bigram[1] in stopWords) \n",
        "               and (len(bigram[0]) > 2) and (len(bigram[1]) > 2)\n",
        "               and not hasNumbers(bigram[0]) and not hasNumbers(bigram[1])])\n",
        "\n",
        "print(bigrams)\n",
        "\n",
        "print('Selected [{}] Bigrams.'.format(len(bigrams)))\n",
        "\n",
        "bi_mwe_tokenizer = MWETokenizer(bigrams)\n",
        "\n",
        "df['bitrigrammed'] = df['trigrammed'].apply(lambda x: bi_mwe_tokenizer.tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBAyzDsVDG8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# restore ngrams into text\n",
        "df['posttext'] = df['bitrigrammed'].apply(lambda toks: [word for word in toks if not word in stopWords])\n",
        "\n",
        "df['posttext'] = df['posttext'].apply(lambda toks: [word for word in toks if not hasNumbers(word)])\n",
        "\n",
        "df['posttext'] = df['posttext'].apply(lambda toks: [word for word in toks if len(word) > 2])\n",
        "\n",
        "\n",
        "df['posttext'] = df['posttext'].apply(lambda toks: [word for word in toks if not word.startswith('_')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_okGfP3DG8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['posttext'] = df['posttext'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9nE9W__DG8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df.drop('text', axis=1, inplace=True)\n",
        "df.drop('pretext', axis=1, inplace=True)\n",
        "df.drop('trigrammed', axis=1, inplace=True)\n",
        "df.drop('bitrigrammed', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc6zMDVDydSu",
        "colab_type": "text"
      },
      "source": [
        "## Split test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ8_AfW6DG8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_label = df['label_i']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJNHkwHMDG8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_docs, x_test_docs, y_train, y_test = train_test_split(df['posttext'], y_label, test_size=0.2, random_state=94606619, stratify=y_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RFnWOoCDG8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.1, min_df=5, analyzer='word')\n",
        "# Fit all the training docs\n",
        "x_train = vectorizer.fit_transform(x_train_docs)\n",
        "# Now use it to process the test docs so they don't influence the training set\n",
        "x_test = vectorizer.transform(x_test_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K0ztc64DG8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vectorizer.vocabulary_.keys())\n",
        "print({ f\"Vocabulary size = {vocab_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWI6HnwNDG8k",
        "colab_type": "text"
      },
      "source": [
        "## Resampling\n",
        "We'll try SMOTe, random oversampling, and a combination over/undersampling with SMOTE - Edited Nearest Neighbours on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtAwTBIFDG8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm = SMOTE(random_state=777, k_neighbors=3)\n",
        "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edOZYGLDDG8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(sm_train_y, density=False, bins=num_labels)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhqn4q4QDG8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm_nn = SMOTEENN(random_state=777, smote=sm)\n",
        "sm_nn_xtrain_tfidf, sm_nn_train_y = sm_nn.fit_sample(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOnc-JjkDG8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(sm_nn_train_y, density=False, bins=num_labels)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WUPvsZADG8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sme = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = sme.fit_resample(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnCoM0QDDG8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(y_res, density=False, bins=num_labels)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NhvNO2YDG8w",
        "colab_type": "text"
      },
      "source": [
        "## Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZg9Wg8ADG8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils func to x-validate a model and record its metrics\n",
        "def score_model(model, x_train, y_train, k_fold):\n",
        "    def balanced_accuracy_fn(y_true, y_pred): return np.mean(balanced_accuracy_score(y_true, y_pred))\n",
        "    def accuracy_fn(y_true, y_pred): return np.mean(accuracy_score(y_true, y_pred))\n",
        "    def precision_fn(y_true, y_pred): return np.mean(precision_score(y_true, y_pred, average=None))\n",
        "    def recall_fn(y_true, y_pred): return np.mean(recall_score(y_true, y_pred, average=None))\n",
        "    def f1_fn(y_true, y_pred): return f1_score(y_true, y_pred, average='macro')\n",
        "    def mcc_fn(y_true, y_pred): return np.mean(matthews_corrcoef(y_true, y_pred))\n",
        "    scoring = {\n",
        "        'accuracy': make_scorer(accuracy_fn),\n",
        "        'balanced_accuracy': make_scorer(balanced_accuracy_fn),\n",
        "        'precision': make_scorer(precision_fn ),\n",
        "        'recall': make_scorer(recall_fn),\n",
        "        'f1': make_scorer(f1_fn),\n",
        "        'mcc': make_scorer(mcc_fn)}\n",
        "\n",
        "    scores = cross_validate(model, x_train, y_train, cv=k_fold, n_jobs=1, scoring = scoring, return_train_score=True )\n",
        "    \n",
        "    result = {\n",
        "        \"xval_test_accuracy\" : np.mean(scores[\"test_accuracy\"]),\n",
        "        \"xval_test_balanced_accuracy\" : np.mean(scores[\"test_balanced_accuracy\"]),        \n",
        "        \"xval_test_precision\": np.mean(scores[\"test_precision\"]),\n",
        "        \"xval_test_recall\": np.mean(scores[\"test_recall\"]),\n",
        "        \"xval_test_f1\": np.mean(scores[\"test_f1\"]),\n",
        "        \"xval_test_mcc\": np.mean(scores[\"test_mcc\"]),\n",
        "        \"xval_train_accuracy\": np.mean(scores[\"train_accuracy\"]),\n",
        "        \"xval_train_precision\": np.mean(scores[\"train_precision\"]),\n",
        "        \"xval_train_recall\": np.mean(scores[\"train_recall\"]),\n",
        "        \"xval_train_mcc\": np.mean(scores[\"train_mcc\"]),\n",
        "        \"model\" : model\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX75SOsSDG82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate test and train metrics for a category; draw a plot of within-category accuracies\n",
        "def fit_assess_model(model, train_x, train_y, test_x, test_y, title):\n",
        "    test_model = best_model_stats[\"model\"].fit(train_x, train_y)\n",
        "\n",
        "    train_preds = test_model.predict(train_x)\n",
        "    test_preds = test_model.predict(test_x)\n",
        "\n",
        "    final_train_accuracy = accuracy_score(train_y, train_preds)\n",
        "    final_test_accuracy = accuracy_score(test_y, test_preds)\n",
        "    final_train_bal_accuracy = balanced_accuracy_score(train_y, train_preds)\n",
        "    final_test_bal_accuracy = balanced_accuracy_score(test_y, test_preds)\n",
        "    final_test_f1 = f1_score(test_y, test_preds, average='macro')\n",
        "    final_train_f1 = f1_score(train_y, train_preds, average='macro')\n",
        "    cm = confusion_matrix(test_y, test_preds)\n",
        "    \n",
        "    print(f\"-------{title} accuracy and f1---------\")\n",
        "    print( \"test accuracy: {}\".format(final_test_accuracy))\n",
        "    print( \"train accuracy: {}\".format(final_train_accuracy))\n",
        "    print( \"test f1: {}\".format( final_test_f1))\n",
        "    print( \"train f1: {}\".format( final_train_f1))\n",
        "\n",
        "    return test_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMo1u_g3DG84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [    \n",
        "  SVC(C=1000, gamma='auto', kernel='rbf', probability=True),\n",
        "  SVC(C=100, gamma='auto', kernel='rbf', probability=True),\n",
        "  SVC(C=10, gamma='auto', kernel='rbf', probability=True),\n",
        "  SVC(C=1, gamma='auto', kernel='rbf', probability=True),\n",
        "  SVC(C=.001, gamma='auto', kernel='rbf',probability=True),\n",
        "  SVC(C=1, gamma='auto', kernel='linear', probability=True)] # linear is not very sensititive to different C values\n",
        "\n",
        "scoredModels = []\n",
        "\n",
        "k_fold = StratifiedKFold(n_splits=5, shuffle=True )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnDRVPEDDG86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose best model using original data without resampling\n",
        "start = datetime.now()\n",
        "\n",
        "for m in models:\n",
        "    print( \"Training model [{}]\".format(m))\n",
        "    scoredModel = score_model(m, x_train, y_train, k_fold)\n",
        "    print('Model metrics: {}.'.format(scoredModel))\n",
        "    scoredModels.append(scoredModel)\n",
        "\n",
        "best_model_stats = max(scoredModels, key=lambda x: x[\"xval_test_accuracy\"])\n",
        "\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"Scored models: {0}\".format(scoredModels))\n",
        "print(\"------------------------------------------------------------------------------------------\")\n",
        "print(\"Chosen model xval stats: {0}\".format(best_model_stats))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSI5ZGrEDG88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a model with all test data without resampling\n",
        "print( \"---------No resampling:-----------\")\n",
        "\n",
        "test_preds = fit_assess_model(best_model_stats[\"model\"], x_train, y_train, x_test, y_test, \"Raw\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHG_RT67W1sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc_by_cat = assess_model(y_test, test_preds, \"SVM\", label_names.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_omjXxLLDG9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now test out the model on the dataset with random resampling\n",
        "print( \"---------Random resampling:-----------\")\n",
        "test_preds = fit_assess_model(best_model_stats[\"model\"], X_res, y_res, x_test, y_test, \"SVM-Random\") \n",
        "\n",
        "ress_acc_by_cat = assess_model(y_test, test_preds, \"SVM-Random\", label_names.tolist())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLTNSX1WDG9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SMOTE resampling\n",
        "print( \"---------SMOTE resampling:-----------\")\n",
        "\n",
        "test_preds = fit_assess_model(best_model_stats[\"model\"],  sm_xtrain_tfidf, sm_train_y, x_test, y_test, \"TF-IDF - SVM SMOTE\") \n",
        "\n",
        "sm_acc_by_cat = assess_model(y_test, test_preds, \"SVM-SMOTE\", label_names.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re1Ihg3CDG9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SMOTEENN resampling\n",
        "print( \"---------SMOTE ENN resampling:-----------\")\n",
        "\n",
        "test_preds = fit_assess_model(best_model_stats[\"model\"],  sm_nn_xtrain_tfidf, sm_nn_train_y, x_test, y_test, \"SVM-SMOTEENN\") \n",
        "\n",
        "smnn_acc_by_cat = assess_model(y_test, test_preds, \"SVM-SMOTEENN\", label_names.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}