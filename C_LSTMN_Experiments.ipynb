{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "C-LSTMN_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZSKtthJNx_a",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Long-Short Term Memory Network Experiments with Resampling\n",
        "\n",
        "## ML Classification for Records Management\n",
        "\n",
        "Jason Franks\n",
        "\n",
        "Master of Data Science Minor Thesis\n",
        "\n",
        "Supervisors: Dr Greg Rolan, Dr Lan Du"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mWDerPPRkX",
        "colab_type": "text"
      },
      "source": [
        "## Install CUDA and SimpleTransformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcoWM221YQDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T_PurTtKa9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv1D,MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import LSTM,Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import nltk as nltk\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, matthews_corrcoef, make_scorer, balanced_accuracy_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, SVMSMOTE, RandomOverSampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.combine import SMOTEENN,SMOTETomek\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvR1FydHKpRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbu-5aJ4PdMV",
        "colab_type": "text"
      },
      "source": [
        "## **Set the following variables to load the data**\n",
        "\n",
        "**mount_path**: path into a google drive to your working folder\n",
        "\n",
        "**data_file**: name of the file containing your data. This must be a tab-separated .tsv file with two columns: 'label', containing the category name, and 'text', containing the record's raw text.\n",
        "\n",
        "Evey category in the data file should have *at least* 10 records.\n",
        "\n",
        "**resample**: set a resampling strategy. Choose from [None, 'random', 'smote', 'smoteenn']\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2cujBKOKysT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mount_path = '/content/drive/My Drive/'\n",
        "all_docs = pd.read_csv( mount_path + \"all_docs_trimmed.tsv\", \"\\t\")\n",
        "resample = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuIWy4m0P3ql",
        "colab_type": "text"
      },
      "source": [
        "## Import and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhPYJUktD-PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isNumber(s):    \n",
        "      try:\n",
        "          float(s)\n",
        "          return True\n",
        "      except ValueError:\n",
        "          return False\n",
        "\n",
        "def hasNumbers(inputString):\n",
        "    return any(char.isdigit() for char in inputString)\n",
        "\n",
        "# Clean text and vectorize\n",
        "def clean_and_drop_stopwords( df, lowercase = False ):\n",
        "  tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
        "  if lowercase:\n",
        "    df['text'] = df['text'].str.lower() \n",
        "\n",
        "  df['pretext'] = df['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "\n",
        "  \n",
        "  nltk.download('stopwords')\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  df['posttext'] = df['pretext'].apply(lambda toks: [word for word in toks if not word in stopWords])\n",
        "\n",
        "  df['posttext'] = df['posttext'].apply(lambda toks: [word for word in toks if not hasNumbers(word)])\n",
        "\n",
        "  df['posttext'] = df['posttext'].apply(lambda toks: [word for word in toks if len(word) > 2])\n",
        "\n",
        "  df['text'] = df['posttext'].apply(lambda x: ' '.join(x))  \n",
        "  df.drop( ['posttext', 'pretext'], axis=1)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw5qcFBYD_vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_docs = clean_and_drop_stopwords(all_docs, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb9_W0RrECMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_names = all_docs['label'].unique()\n",
        "all_docs['label_i'] = all_docs['label'].astype('category').cat.codes\n",
        "\n",
        "y_label = all_docs['label_i']\n",
        "labels = all_docs['label_i'].unique()\n",
        "num_labels = len(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ciwvEHpEFex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions to help assess the output\n",
        "\n",
        "def get_within_category_accuracies( cat_list, cm ):\n",
        "    cat_accuracies = []\n",
        "    for row in range(len(cat_list)):\n",
        "        cm_row = cm[row]\n",
        "        num_correct = cm_row[row]\n",
        "\n",
        "        total = sum(cm[row])\n",
        "        if total == 0:\n",
        "            continue\n",
        "        \n",
        "        cat_accuracies.append(num_correct/total)\n",
        "            \n",
        "    df = pd.DataFrame(zip(cat_list, cat_accuracies), columns=['label', 'accuracy'])\n",
        "    return df\n",
        "\n",
        "def assess_model(test, preds, title, labels, draw_plot=True):        \n",
        "    final_test_accuracy = accuracy_score(test, preds)\n",
        "    final_test_f1 = f1_score(test, preds, average='macro') \n",
        "    final_cat_f1s = f1_score(test, preds, average=None) \n",
        "    final_test_f1_weighted = f1_score(test, preds, average='weighted')    \n",
        "    final_test_precision = precision_score(test, preds, average='macro') \n",
        "    final_cat_precision = precision_score(test, preds, average=None) \n",
        "    final_test_precision_weighted = precision_score(test, preds, average='weighted')    \n",
        "    final_test_recall = recall_score(test, preds, average='macro') \n",
        "    final_cat_recall = recall_score(test, preds, average=None) \n",
        "    final_test_recall_weighted = recall_score(test, preds, average='weighted')    \n",
        "    cm = confusion_matrix(test, preds)\n",
        "\n",
        "    metrics=[]\n",
        "    metrics.append( [\"accuracy\", final_test_accuracy])\n",
        "    metrics.append( [\"f1\", final_test_f1])\n",
        "    metrics.append( [\"f1 weighted\", final_test_f1_weighted])\n",
        "    metrics.append( [\"precision\", final_test_precision])\n",
        "    metrics.append( [\"precision weighted\", final_test_precision_weighted])\n",
        "    metrics.append( [\"recall\", final_test_recall])\n",
        "    metrics.append( [\"recall weighted\", final_test_recall_weighted])\n",
        "\n",
        "    print( \"------------Model assessment-----\")\n",
        "\n",
        "    print( \"test f1 / category, {}\\n\".format( final_cat_f1s))   \n",
        "    print( \"test precision / category, {}\\n\".format( final_cat_precision))   \n",
        "    print( \"test recall / category, {}\\n\".format( final_cat_recall))   \n",
        "    \n",
        "    model_assessment = pd.DataFrame(metrics, columns=[\"metric\", \"value\"])\n",
        "    print(model_assessment)\n",
        "    model_assessment.to_csv(f'{mount_path}/{title}_assess.csv', index=False )\n",
        "\n",
        "    acc_by_cat = get_within_category_accuracies( labels, cm)\n",
        "\n",
        "    acc_by_cat.to_csv(f'{mount_path}/output/{title}_acc_by_cat.csv', index=False )\n",
        "\n",
        "    if draw_plot:\n",
        "      ax = acc_by_cat.plot.bar( x='label', y='accuracy', title=f'{title} Accuracy by Category', legend=None, figsize=(20,20), fontsize=14)\n",
        "      ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "      ax.set_xticklabels(labels, rotation=90, fontsize=12)\n",
        "      plt.tight_layout()\n",
        "      fig = ax.get_figure()\n",
        "      fig.savefig(mount_path + f'/output/{title}_Accuracy_by_Category.png', dpi=300)\n",
        "    \n",
        "    print(\"-------Confusion Matrix---------\")\n",
        "    print(cm)\n",
        "    \n",
        "    cmDF = pd.DataFrame.from_records(cm)    \n",
        "    cmDF.columns=labels\n",
        "    cmDF.index=labels\n",
        "    cmDF.to_csv(f'{mount_path}/output/{title}_cm.csv', index=True)\n",
        "\n",
        "    return acc_by_cat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzhf8SdtNZh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encode the labels\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(all_docs['label_i'])\n",
        "encoded_Y = encoder.transform(all_docs['label_i'])\n",
        "\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "dummy_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJguYiJ0WLBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_docs['text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "encoded_X = tokenizer.texts_to_sequences(all_docs['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PcGcfQeO79X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_words = 10000 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MJp0wxNLbUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_all, X_test, y_train_all, y_test = train_test_split(encoded_X, dummy_y, test_size=0.2, random_state=94606619, stratify=dummy_y)\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X_train_all, y_train_all, test_size = 0.2, random_state=94606619, stratify=y_train_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM9rtrDhKa9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_document_length = 15000\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_document_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_document_length)\n",
        "X_cv = sequence.pad_sequences(X_cv,maxlen=max_document_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoIDZVwd7zOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_max = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bacyihmHklvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if resample == 'random':\n",
        "  sme = RandomOverSampler(random_state=42)\n",
        "  X_train, y_train = sme.fit_resample(X_train, y_train)\n",
        "elif resample == 'smote':\n",
        "  sm = SMOTE(random_state=777, k_neighbors=3)\n",
        "  X_train, y_train = sm.fit_sample(X_train, y_train)\n",
        "elif resample == 'smoteenn':\n",
        "  sm = SMOTE(random_state=777, k_neighbors=3)\n",
        "  sm_nn = SMOTEENN(random_state=777, smote=sm)\n",
        "  X_train, y_train = sm_nn.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HckuXcgwF3Ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_vector_length = 64\n",
        "num_epochs = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujajvSDZF92Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "starttime = datetime.now()\n",
        "starttime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xktp-ErmKa9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# LSTM + CNN\n",
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_document_length))\n",
        "model.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)) # filters=32\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(250))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "filepath=\"weights_best_cnn.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
        "callbacks_list = [checkpoint]\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=256,verbose = 1,callbacks = callbacks_list,validation_data=(X_cv,y_cv))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE-RNJA6GPp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "endtime = datetime.now()\n",
        "endtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ3v6BIiKa9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_gsNdPzid0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnVJwfBikvUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title = \"C-LSTM\"\n",
        "if resample != None:\n",
        "  title = f\"C-LSTM-{resample}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc3UlbnO_JRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_max = np.argmax(y_pred, axis=1)\n",
        "acc_by_cat = assess_model(y_test_max, y_pred_max, title, label_names.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}